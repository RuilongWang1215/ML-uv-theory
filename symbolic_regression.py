# This script is the symbolic regression framework for modelling the correction term of uv-theory
import numpy as np
import pandas as pd
import time
from pysr import PySRRegressor
import sympy as sp
# import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import os
import numpy as np


# This function is used to organize the files generated by the symbolic regression
'''It will move the generated files to the 'result' folder and rename them
The naming convention is 'uv_trial_{substance}_{count}.csv'
The count is the number of files with the same substance name'''
def organize_files(name):
    file_names = os.listdir()
    file_in_result = os.listdir('result')
    counts = {}
    for file in file_in_result:
        if file.startswith(name):
            counts[file] = file.split('_')[-1].split('.')[0]
    if counts:
        max_count = max([int(count) for count in counts.values()])
        name = name + '_' + str(max_count + 1)

    for file_name in file_names:
        if file_name.startswith('hall_of_fame'):
            if file_name.endswith('.csv'):
                os.replace(file_name, 'result/' + name + '.csv')
            if file_name.endswith('.pkl'):
                os.replace(file_name, 'result/' + name +'.pkl')
            if file_name.endswith('.bkup'):
                os.remove(file_name)

# This function is used to plot the regression results
def plot_regression(X_test, y_test, model, substance):
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    fig = plt.figure()
    plt.plot(y_test, y_pred, 'o', color='skyblue')
    plt.plot(y_test, y_test, 'slategrey')
    plt.text(0.7, 0.2, f'R2: {r2:.2f}', transform=fig.transFigure)
    plt.xlabel('True')
    plt.ylabel('Predicted')
    plt.title(f"True vs Predicted {substance} Correction Term")
    plt.savefig(f"result/{substance}_{r2}.png", dpi=300, bbox_inches='tight')

# This function is used to sample the argon data, since the data is too large
def argon_sample(argon, sample_frac):
    argon_sample = pd.DataFrame({})
    Temperatures = argon['temperature'].unique()
    for T in Temperatures:
        if T < 200:
            data = argon[argon['temperature'] == T]
            argon_sample = pd.concat([argon_sample, data])
        if T>=200:
            data = argon[argon['temperature'] == T].sample(frac = sample_frac)
            argon_sample = pd.concat([argon_sample, data])
    print("length of original data: ", len(argon), "length of sampled data: ", len(argon_sample))
    return argon_sample

# This function is used to run the symbolic regression
'''It will return the model fitted with the training data'''
def run_SR(X, y, iteration, substance):
    if substance == 'argon':
        BATCHING = True
    else:
        BATCHING = False
    model = PySRRegressor(
        niterations=iteration,
        procs=48,  # Increase me for better results
        populations=48*3,
        maxsize = 30,
        binary_operators=["+", "*", "-", "/"],
        unary_operators=[
            "cos",
            "sin",
            "exp",
            "tan",
            "inv(x) = 1/x",
            "sqrt",
            "log",
            "square",
            "cube",
            "sinh(x) = (exp(x) - exp(-x))/2",
            "cosh(x) = (exp(x) + exp(-x))/2",
            # ^ Custom operator (julia syntax)
        ],
        extra_sympy_mappings={"inv": lambda x: 1 / x,
                              "sinh": lambda x: (sp.exp(x) - sp.exp(-x)) / 2,
                                "cosh": lambda x: (sp.exp(x) + sp.exp(-x)) / 2},
        # ^ Define operator for SymPy as well
        elementwise_loss="loss(prediction, target) = (prediction - target)^2",
        ncycles_per_iteration = 1000,
        timeout_in_seconds = 60*60*8,
        progress= True,
        nested_constraints= {'tan':{'tan': 0, 'cos': 1},
                          'cos':{'tan': 1, 'cos': 0}},
        batching = BATCHING,
        bumper = True,
                             )
    
    model.fit(X, y)
    return model

def detect_outliers(data, threshold=3):
    mean = np.mean(data['delta_phi'])
    std_dev = np.std(data['delta_phi'])
    outliers = []
    for i in data['delta_phi']:
        z_score = (i - mean) / std_dev
        if np.abs(z_score) > threshold:
            outliers.append(i)
    data_without_outliers = data[~data['delta_phi'].isin(outliers)]
    return data_without_outliers

substance = 'water'
data = pd.read_csv(f'data/real_ufrac_{substance}.csv')
if substance == 'argon':
    data = argon_sample(data, 0.75)
'''if substance == 'water':
    data = detect_outliers(data)'''
X = data.iloc[:, 1:3]
y = data['delta_phi']
NAME = 'uv_trial_' + substance
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)
model = run_SR(X_train, y_train, iteration = 10000, substance = substance)
plot_regression(X_test, y_test, model, substance)
organize_files(name=NAME)
